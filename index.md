[Home](/index.md) &nbsp; &nbsp; [Research](/research.md)

I am a final year PhD student at the University of California, Berkeley.
I am broadly interested in theoretical machine learning. My current focus is in optimization, where I am mainly working from the perspective of continuous-time. Active projects include using ideas from numerical integration to develop computationally efficient adaptive step size schemes for optimizers with well-defined continuous-time (ODE) representations, and examining the effect of overparameterization on the dynamics of stochastic gradient descent in matrix factorization problems.
Some of my other interests are the statistical mechanics of small systems out of equilibrium, and the three-way interface of computer science, statistics, and statistical mechanics.

My advisors are [Michael I. Jordan](http://people.eecs.berkeley.edu/~jordan/) and [Michael R. DeWeese](https://deweeselab.com/).
I am affiliated with the Statistical AI Learning group, the [Berkeley AI Research](https://bair.berkeley.edu/) group, and the [Redwood Center for Theoretical Neuroscience](https://redwood.berkeley.edu).

In the summer of 2019 I interned at Google Brain, where I was hosted by [Jascha Sohl-Dickstein](http://www.sohldickstein.com/). During the academic years 2018-21, I was supported by a Google PhD Fellowship.

Before I came to Berkeley, I was a Junior Research Fellow at the National Center for Biological Sciences in Bangalore, India. Before that, I completed a Masters degree in theoretical physics at the Perimeter Institute for Theoretical Physics in Waterloo, Canada. I was an undergraduate at Amherst College, where I received a degree in physics.

You can find me at neha _dot_ wadia _at_ berkeley _dot_ edu.\
Here is my [Google Scholar](https://scholar.google.com/citations?hl=en&user=5qC5g3MAAAAJ) page.
