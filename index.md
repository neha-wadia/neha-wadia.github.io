[Home](/index.md) &nbsp; &nbsp; [Research](/research.md)

I am a postdoctoral fellow at the [Center for Computational Mathematics](https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/) of the Flatiron Institute. I am broadly interested in the theory of machine learning. I work mainly on problems in optimization, sampling, and inference. Active projects include using ideas from numerical integration to develop computationally efficient adaptive step size schemes for optimization, and studying the mixing time of the Gibbs sampler on log-concave distributions. I have a new [preprint](https://arxiv.org/abs/2412.17899) out on the latter.

I graduated with a PhD from the University of California, Berkeley in May of 2022. My advisors were [Michael I. Jordan](http://people.eecs.berkeley.edu/~jordan/) and [Michael R. DeWeese](https://deweeselab.com/).
I was also affiliated with the Statistical AI Learning group, the [Berkeley AI Research](https://bair.berkeley.edu/) group, and the [Redwood Center for Theoretical Neuroscience](https://redwood.berkeley.edu).

In the summer of 2019 I interned at Google Brain, where I was hosted by [Jascha Sohl-Dickstein](http://www.sohldickstein.com/). During the academic years 2018-21, my work was supported by a [Google PhD Fellowship](https://research.google/outreach/phd-fellowship/recipients/).

Before I went to Berkeley, I was a Junior Research Fellow at the National Center for Biological Sciences in Bangalore, India. Before that, I completed a Masters degree in theoretical physics at the Perimeter Institute for Theoretical Physics in Waterloo, Canada. I was an undergraduate at Amherst College, where I received a degree in physics.

You can find me at nwadia _at_ flatironinstitute _dot_ org.\
Here is my [Google Scholar](https://scholar.google.com/citations?hl=en&user=5qC5g3MAAAAJ) page.

Machine learning at the Flatiron Institute has its own webpage [here](https://www.simonsfoundation.org/machine-learning-at-the-flatiron-institute/).
